{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Dense,Dropout,Conv1D,LSTM,MaxPooling1D,GlobalAveragePooling1D\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../glove.6B.50d.txt',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dictionary with the word as the key and 50 dimensional embedding as it's value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs =np.asarray(values[1:],dtype='float')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load train and labels\n",
    "data = np.load('train.npy')\n",
    "labels = np.load('labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "labels = to_categorical(labels,num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into training data and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(data,labels,random_state=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15600,), (5200,), (15600, 2), (5200, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3257,\n",
       " 3287,\n",
       " 3400,\n",
       " 3489,\n",
       " 3570,\n",
       " 3570,\n",
       " 3637,\n",
       " 3644,\n",
       " 3682,\n",
       " 3715,\n",
       " 3725,\n",
       " 3760,\n",
       " 3796,\n",
       " 3924,\n",
       " 3933,\n",
       " 3936,\n",
       " 3955,\n",
       " 4076,\n",
       " 4154,\n",
       " 4257,\n",
       " 4274,\n",
       " 4283,\n",
       " 4291,\n",
       " 4291,\n",
       " 4304,\n",
       " 4350,\n",
       " 4435,\n",
       " 4498,\n",
       " 4513,\n",
       " 4515,\n",
       " 4737,\n",
       " 4760,\n",
       " 4815,\n",
       " 4821,\n",
       " 4885,\n",
       " 5114,\n",
       " 5162,\n",
       " 5190,\n",
       " 5546,\n",
       " 5593,\n",
       " 5799,\n",
       " 6717,\n",
       " 6742,\n",
       " 7686,\n",
       " 7849,\n",
       " 8854,\n",
       " 9196,\n",
       " 10422,\n",
       " 10696,\n",
       " 12185]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = [len(x) for x in x_train]\n",
    "mean.sort()\n",
    "mean[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the most of the values are within 3000 words mark. Therefore while training the model, we'll take the max number of words in the embedding to be 3000 so as to reduce the computation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the words in our corpus to embeddings obtained from the Glove Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_output(X):\n",
    "    emb_dim = 50\n",
    "    maxLen = 3000\n",
    "    \n",
    "    embedding_output = np.zeros((len(X),maxLen,emb_dim)) #(batch_size,max len of sentence,embedding dimension)\n",
    "    for i in range(len(X)):\n",
    "        for j in range(min(len(X[i]),3000)):\n",
    "            try:\n",
    "                embedding_output[i][j] = embedding_index[X[i][j]]\n",
    "            except:\n",
    "                embedding_output[i][j] = np.zeros((50,))\n",
    "    return embedding_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-a739a47a9a60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membedded_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0membedded_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-88da515ab1db>\u001b[0m in \u001b[0;36membedding_output\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmaxLen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0membedding_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaxLen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0memb_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#(batch_size,max len of sentence,embedding dimension)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedded_train = embedding_output(x_train)\n",
    "embedded_test = embedding_output(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things that can be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we know, the number of words in a text example is quite high, therefore simply using LSTM might take some time.\n",
    "We can do the following to reduce computation time:\n",
    "\n",
    "- use CuDNN LSTM\n",
    "- use LSTM + 1D CNN. Here the text is passed through a CNN layer. Depending on the stride chosen, the features are reduced.(i.e if we choose stride as 3 and we have 3k features, the resulting output of this layer would be 1k, therefore the data is reduced by 3 times). Then pass the output through the LSTM layer. I tried this type of model for this problem, but i was getting a pretty low accuracy\n",
    "- Simply use 1D CNN. As our data is just a binary classification, it wouldn't matter a lot if\n",
    "we lose the dependencies between the data. Moreover, using cnn will also prevent the problem of vanishing gradients(if present)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 2998, 64)          9664      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 999, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 999, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 997, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 332, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 330, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 110, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 110, 64)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 42,946\n",
      "Trainable params: 42,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(64,3,input_shape=(3000,50),activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64,3,activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Conv1D(64,3,activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(LSTM(128))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12480 samples, validate on 3120 samples\n",
      "Epoch 1/2\n",
      "12480/12480 [==============================] - 160s 13ms/step - loss: 0.2415 - acc: 0.9024 - val_loss: 0.2070 - val_acc: 0.9141\n",
      "Epoch 2/2\n",
      "12480/12480 [==============================] - 171s 14ms/step - loss: 0.1939 - acc: 0.9238 - val_loss: 0.1849 - val_acc: 0.9256\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "#checkpoint = ModelCheckpoint(\"model.h5\", monitor='val_loss', verbose=1, save_best_only=True, period=1)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "hist = model.fit(embedded_train,y_train,validation_split=0.2,epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5200/5200 [==============================] - 30s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2011418330210906, 0.9207692307692308]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(embedded_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Testing accuracy: .92\n",
    "Can be improved my training our model for more epochs and tuning the hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
